# -*- coding: utf-8 -*-
"""MiniProyecto#3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17AkQz7YBZFXfkPXeU5OyRvi-lnXVJ9f0

## Librerias
"""

import os
import cv2
import numpy as np
import pandas as pd
from skimage.feature import hog, local_binary_pattern
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import random

"""## Google Drive"""

import os
from datetime import date

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

current_dir = "/content/drive/My Drive/MiniProyecto2"

if not os.path.isdir(current_dir):
  os.makedirs(current_dir)

current_dir += '/'

"""## Descargar dataset"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("kmader/skin-cancer-mnist-ham10000")

print("Path to dataset files:", path)

print("Contents of the dataset directory:")
print(os.listdir(path))
image_path1 = path + '/HAM10000_images_part_1'
print(os.listdir(image_path1))
image_path2 = path + '/HAM10000_images_part_2'
print(os.listdir(image_path2))

from PIL import Image

image_file = os.path.join(path+"/HAM10000_images_part_1", "ISIC_0025151.jpg")
image = Image.open(image_file)

image_resized = image.resize((128, 128))

plt.imshow(image_resized)  # Display the image

image_file = os.path.join(path, "hmnist_8_8_RGB.csv")
data_8_8_label = pd.read_csv(image_file)['label']
data_8_8_label

import os
import numpy as np
import pandas as pd
from PIL import Image

def image_to_csv(image_folder1, image_folder2, output_csv, labels):
    """
    Converts images in a folder to a CSV file with flattened RGB data.

    Parameters:
        image_folder (str): Path to the folder containing images.
        output_csv (str): Path to save the resulting CSV file.
    """
    data = []
    for i, image_name in enumerate(os.listdir(image_folder1)):
        image_path = os.path.join(image_folder1, image_name)
        try:
            # Open and resize the image to 8x8
            image = Image.open(image_path).resize((64, 64))

            # Convert the image to an array (8x8x3)
            image_array = np.array(image)

            # Flatten the array to 1D (8*8*3 = 192 for RGB)
            flattened = image_array.flatten()

            # Optionally add a label column
            #if label_mapping and image_name in label_mapping:
            #    flattened = np.append(flattened, label_mapping[image_name])
            flattened = np.append(flattened, labels[i])

            # Append the flattened image data
            data.append(flattened)
        except Exception as e:
            print(f"Error processing image {image_name}: {e}")

    for i, image_name in enumerate(os.listdir(image_folder2)):
        image_path = os.path.join(image_folder2, image_name)
        try:
            # Open and resize the image to 8x8
            image = Image.open(image_path).resize((64, 64))

            # Convert the image to an array (8x8x3)
            image_array = np.array(image)

            # Flatten the array to 1D (8*8*3 = 192 for RGB)
            flattened = image_array.flatten()

            # Optionally add a label column
            #if label_mapping and image_name in label_mapping:
            #    flattened = np.append(flattened, label_mapping[image_name])
            flattened = np.append(flattened, labels[i+len(os.listdir(image_folder1))])

            # Append the flattened image data
            data.append(flattened)
        except Exception as e:
            print(f"Error processing image {image_name}: {e}")

    # Create column names
    num_pixels = 64 * 64 * 3  # For RGB
    columns = [f"pixel_{i}" for i in range(num_pixels)]
    columns.append("label")

    # Convert to DataFrame
    df = pd.DataFrame(data, columns=columns)

    # Save to CSV
    df.to_csv(output_csv, index=False)
    print(f"CSV file saved to {output_csv}")

# Example Usage
output_csv = "hmnist_64_64.csv"       # Output CSV file
#label_mapping = {"image1.jpg": 0, "image2.jpg": 1}  # Optional labels
labels = data_8_8_label

image_to_csv(image_path1, image_path2, output_csv, labels)

"""## Extracción de características"""

# Parámetros de HOG
hog_params = {'orientations': 9, 'pixels_per_cell': (8, 8), 'cells_per_block': (2, 2), 'block_norm': 'L2-Hys'}

# Parámetros de LBP
radius = 3
n_points = 8 * radius
method = 'uniform'

# Función para leer el CSV y convertir los datos en imágenes
def load_images_from_csv(csv_path):
    # Cargar el CSV
    data = pd.read_csv(csv_path)

    # Separar las etiquetas (si están presentes)
    if 'label' in data.columns:
        labels = data['label']
        data = data.drop('label', axis=1)
    else:
        labels = None

    # Convertir los valores de los píxeles en una matriz numpy y redimensionar a imágenes 28x28 RGB
    images = data.values.reshape(-1, 64, 64, 3).astype(np.uint8)  # Cambiar tipo a uint8 para compatibilidad
    return images, labels

# Extraer características HOG y LBP
def extract_features(images):
    hog_images = []
    hog_features = []
    lbp_features = []
    for img in images:
        # Asegurarse de que la imagen esté en formato uint8
        img = img.astype(np.uint8)
        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convertir a escala de grises

        # Extraer HOG
        hog_feat,hog_image = hog(gray_img, **hog_params,visualize=True)
        hog_features.append(hog_feat)
        hog_images.append(hog_image)

        # Extraer LBP
        lbp = local_binary_pattern(gray_img, n_points, radius, method)
        lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))
        lbp_hist = lbp_hist.astype("float")
        lbp_hist /= (lbp_hist.sum() + 1e-6)  # Normalizar
        lbp_features.append(lbp_hist)

    return np.array(hog_features), np.array(lbp_features) , np.array(hog_images)

# Aplicar PCA para reducir la dimensionalidad
def apply_pca(features, n_components=50):
    pca = PCA(n_components=n_components)
    pca_features = pca.fit_transform(features)
    return pca_features

# Ruta al archivo CSV con las imágenes
csv_file_path = 'hmnist_64_64.csv'

# Cargar las imágenes del CSV
images, labels = load_images_from_csv(csv_file_path)

# Extraer características HOG y LBP
hog_features, lbp_features, hog_images = extract_features(images)

# Combinar HOG y LBP
combined_features = np.hstack((hog_features, lbp_features))
print(f"Dimensiones de las características combinadas: {combined_features.shape}")

# Aplicar PCA para reducir la dimensionalidad a 50 componentes
pca_features = apply_pca(combined_features, n_components=50)

# Guardar las características procesadas y las etiquetas (si están presentes)
np.save('features_pca.npy', pca_features)
if labels is not None:
    np.save('labels.npy', labels)

print("Extracción de características y PCA completados. Los resultados se han guardado en 'features_pca.npy' y 'labels.npy'.")

hog_features.shape, hog_images.shape

"""# HOG : Histogramas de Gradientes Orientados"""

# Seleccionar 15 índices aleatorios
random_indices = random.sample(range(len(images)), 15)

# Crear una figura con subgráficos
fig, axes = plt.subplots(5, 6, figsize=(15, 10))

for i, idx in enumerate(random_indices):
    row = i // 3
    col = (i % 3) * 2

    axes[row, col].imshow(images[idx])
    axes[row, col].set_title(f"Original {idx}")
    axes[row, col].axis('off')

    gray_img = cv2.cvtColor(images[idx], cv2.COLOR_RGB2GRAY)  # Convert image to grayscale
    _, hog_image = hog(gray_img, visualize=True, **hog_params)  # Get HOG features and visualization
    axes[row, col + 1].imshow(hog_image)
    axes[row, col + 1].set_title(f"HOG {idx}")
    axes[row, col + 1].axis('off')

plt.tight_layout()
plt.show()

"""# PCA: Análisis de Componentes Principales"""

data_values = hog_features

pca = PCA(n_components=data_values.shape[1], random_state=42)
pca.fit_transform(data_values)

y = np.cumsum(pca.explained_variance_ratio_)
componente_90 = np.where(y > 0.90)[0][0]
print(f'El número de componentes para el 90% de la varianza explicada es {componente_90}.')

plt.axvline(componente_90, linestyle='--', color='green')
plt.plot(y)
plt.grid()

data_values = hog_features
data_values.shape

pca = PCA(componente_90, random_state=42)
pca_features = pca.fit_transform(data_values)
df = pd.DataFrame(pca_features)
df.to_csv('pca_hmnist_64_64.csv')
df.to_csv(current_dir + 'pca_hmnist_64_64.csv')

df

"""# Clasificadores

### Instalaciones
"""

!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com

"""## SVM: Máquina de soporte vectorial

### Imágenes originales
"""

from cuml.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report

# Ruta al archivo CSV con las imágenes
csv_file_path = 'hmnist_64_64.csv'

# Cargar las imágenes del CSV
images, labels = load_images_from_csv(current_dir + csv_file_path)
flat_size = 64*64*3
images_flattened = images.reshape((images.shape[0], flat_size))

images_flattened = images_flattened.astype('float32')
labels = labels.astype('int32')
print(images_flattened.dtype)  # Tipo de datos de X
print(labels.dtype)  # Tipo de datos de y

X_train, X_test, y_train, y_test = train_test_split(images_flattened, labels, test_size=0.3, random_state=42)

#param_grid = {
#    'C': [0.1, 1, 10, 100],
#    'gamma': ['scale', 'auto', 0.01, 0.001],
#    'kernel': ['rbf']
#}

#grid = GridSearchCV(SVC(class_weight='balanced'), param_grid, cv=3)
#grid.fit(X_train, y_train)
svc = SVC(kernel='rbf', C=1, gamma='scale', class_weight='balanced')
svc.fit(X_train, y_train)

#y_pred = grid.best_estimator_.predict(X_test)
y_pred = svc.predict(X_test)

np.unique(y_pred, return_counts=True)

print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

etiquetas = ['NV', 'MEL', 'BKL', 'BCC', 'AKIEC', 'VASC', 'DF']

C = confusion_matrix(y_test, y_pred)
sns.heatmap(C, annot=True, fmt='d', cmap='Greens', xticklabels=np.unique(etiquetas), yticklabels=np.unique(etiquetas))
plt.title('Matriz de confusión')
plt.xlabel('Etiquetas')
plt.ylabel('Etiquetas');

"""### Features con PCA"""

# Ruta al archivo CSV con las imágenes
csv_file_path = 'pca_hmnist_64_64.csv'

# Cargar las imágenes del CSV
pca_features = pd.read_csv(current_dir + csv_file_path)

X_train, X_test, y_train, y_test = train_test_split(pca_features, labels, test_size=0.3, random_state=42)

#param_grid = {
#    'C': [0.1, 1, 10, 100],
#    'gamma': ['scale', 'auto', 0.01, 0.001],
#    'kernel': ['rbf']
#}

#grid = GridSearchCV(SVC(class_weight='balanced'), param_grid, cv=3)
#grid.fit(X_train, y_train)
svc = SVC(kernel='rbf', C=1, gamma='scale', class_weight='balanced')
svc.fit(X_train, y_train)

#y_pred = grid.best_estimator_.predict(X_test)
y_pred = svc.predict(X_test)

np.unique(y_pred, return_counts=True)

print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

etiquetas = ['NV', 'MEL', 'BKL', 'BCC', 'AKIEC', 'VASC', 'DF']

C = confusion_matrix(y_test, y_pred)
sns.heatmap(C, annot=True, fmt='d', cmap='Greens', xticklabels=np.unique(etiquetas), yticklabels=np.unique(etiquetas))
plt.title('Matriz de confusión')
plt.xlabel('Etiquetas')
plt.ylabel('Etiquetas');

"""El SVM con PCA muestra un rendimiento significativamente mejorado, alcanzando un accuracy del 90% y métricas equilibradas (precision, recall, F1-score), gracias a la reducción de dimensionalidad que simplificó la representación de los datos y eliminó ruido innecesario.

**Comparativa:**
En comparación con el SVM sin PCA (con imágenes originales), donde las métricas fueron mucho más bajas debido a la alta dimensionalidad y falta de preprocesamiento, el uso de PCA permitió al modelo manejar mejor la complejidad, resultando en fronteras de decisión más precisas y un mejor desempeño global.

## Regresión logística con Softmax

### Imágenes originales
"""

import tensorflow as tf
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report
from sklearn.utils.class_weight import compute_class_weight

# Ruta al archivo CSV con las imágenes
csv_file_path = 'hmnist_64_64.csv'

# Cargar las imágenes del CSV
images, labels = load_images_from_csv(current_dir + csv_file_path)
flat_size = 64*64*3
images_flattened = images.reshape((images.shape[0], flat_size))

images_flattened = images_flattened.astype('float32')  / 255.0
labels = labels.astype('int32').values
print(images_flattened.dtype)  # Tipo de datos de X
print(labels.dtype)  # Tipo de datos de y

print('labels')
print(labels)

X_train, X_test, y_train, y_test = train_test_split(images_flattened, labels, test_size=0.3, random_state=42)

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(flat_size,)),  # Tamaño de entrada
    tf.keras.layers.Dense(7, activation='softmax')  # 7 clases (salida multiclase)
])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}
print(class_weight_dict)

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

np.unique(y_pred_classes, return_counts=True)

print(classification_report(y_test, y_pred_classes))

from sklearn.metrics import confusion_matrix
import seaborn as sns

etiquetas = ['NV', 'MEL', 'BKL', 'BCC', 'AKIEC', 'VASC', 'DF']

C = confusion_matrix(y_test, y_pred_classes)
sns.heatmap(C, annot=True, fmt='d', cmap='Greens', xticklabels=np.unique(etiquetas), yticklabels=np.unique(etiquetas))
plt.title('Matriz de confusión')
plt.xlabel('Etiquetas')
plt.ylabel('Etiquetas');

"""### Features con PCA"""

# Ruta al archivo CSV con las imágenes
csv_file_path = 'pca_hmnist_64_64.csv'

# Cargar las imágenes del CSV
pca_features = pd.read_csv(current_dir + csv_file_path)
print('Shape:', pca_features.shape)

X_train, X_test, y_train, y_test = train_test_split(pca_features, labels, test_size=0.3, random_state=42)

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(pca_features.shape[1],)),  # Tamaño de entrada
    tf.keras.layers.Dense(7, activation='softmax')  # 7 clases (salida multiclase)
])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}
print(class_weight_dict)

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

print(classification_report(y_test, y_pred_classes))

from sklearn.metrics import confusion_matrix
import seaborn as sns

etiquetas = ['NV', 'MEL', 'BKL', 'BCC', 'AKIEC', 'VASC', 'DF']

C = confusion_matrix(y_test, y_pred_classes)
sns.heatmap(C, annot=True, fmt='d', cmap='Greens', xticklabels=np.unique(etiquetas), yticklabels=np.unique(etiquetas))
plt.title('Matriz de confusión')
plt.xlabel('Etiquetas')
plt.ylabel('Etiquetas');

"""El uso de PCA mejoró el desempeño en clases como 2, pero redujo el desempeño global debido a la dificultad de la Regresión Logística para manejar datos transformados con pérdida de características críticas. Esto significa que, para datasets grandes y complejos como este, la Regresión Logística no es el modelo ideal.

## Regresión logística con OvR

### Imágenes originales
"""

import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Ruta al archivo CSV con las imágenes
csv_file_path = 'hmnist_64_64.csv'

# Cargar las imágenes del CSV
images, labels = load_images_from_csv(current_dir + csv_file_path)
flat_size = 64 * 64 * 3
images_flattened = images.reshape((images.shape[0], flat_size)).astype('float32') / 255.0
labels = labels.astype('int32')

# Verificar los datos
print("Tipo de datos de imágenes:", images_flattened.dtype)
print("Tipo de datos de etiquetas:", labels.dtype)
print("Clases únicas:", np.unique(labels))

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(images_flattened, labels, test_size=0.3, random_state=42)

# Crear y entrenar modelos OvR
unique_classes = np.unique(y_train)
models = {}

for cls in unique_classes:
    print(f"Entrenando modelo para la clase {cls}...")

    # Crear etiquetas binarizadas (1 para la clase actual, 0 para las demás)
    y_train_binary = (y_train == cls).astype('float32').values

    # Calcular los pesos para la clase actual
    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=np.array([0, 1]),
        y=y_train_binary
    )
    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}

    # Definir el modelo binario para la clase
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(flat_size,)),  # Tamaño de entrada
        tf.keras.layers.Dense(1, activation='sigmoid')  # Salida binaria
    ])

    # Compilar el modelo
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    # Entrenar el modelo
    model.fit(
        X_train,
        y_train_binary,
        epochs=10,
        batch_size=32,
        validation_split=0.2,
        class_weight=class_weight_dict,
        verbose=1
    )

    models[cls] = model

class_scores = []
for cls in unique_classes:
    print(f"Prediciendo para la clase {cls}...")
    class_scores.append(models[cls].predict(X_test, verbose=0).flatten())

class_scores = np.array(class_scores)

y_pred = np.argmax(class_scores, axis=0)

# Evaluar el modelo
print("Reporte de clasificación:")
print(classification_report(y_test, y_pred))

np.unique(y_pred, return_counts=True)

from sklearn.metrics import confusion_matrix
import seaborn as sns

etiquetas = ['NV', 'MEL', 'BKL', 'BCC', 'AKIEC', 'VASC', 'DF']

C = confusion_matrix(y_test, y_pred)
sns.heatmap(C, annot=True, fmt='d', cmap='Greens', xticklabels=np.unique(etiquetas), yticklabels=np.unique(etiquetas))
plt.title('Matriz de confusión')
plt.xlabel('Etiquetas')
plt.ylabel('Etiquetas');

"""### Features con PCA"""

# Ruta al archivo CSV con las imágenes
csv_file_path = 'pca_hmnist_64_64.csv'

# Cargar las imágenes del CSV
pca_features = pd.read_csv(current_dir + csv_file_path)
print('Shape:', pca_features.shape)

X_train, X_test, y_train, y_test = train_test_split(pca_features, labels, test_size=0.3, random_state=42)

# Crear y entrenar modelos OvR
unique_classes = np.unique(y_train)
models = {}

for cls in unique_classes:
    print(f"Entrenando modelo para la clase {cls}...")

    # Crear etiquetas binarizadas (1 para la clase actual, 0 para las demás)
    y_train_binary = (y_train == cls).astype('float32').values

    # Calcular los pesos para la clase actual
    class_weights = compute_class_weight(
        class_weight='balanced',
        classes=np.array([0, 1]),
        y=y_train_binary
    )
    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}

    # Definir el modelo binario para la clase
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(pca_features.shape[1],)),  # Tamaño de entrada
        tf.keras.layers.Dense(1, activation='sigmoid')  # Salida binaria
    ])

    # Compilar el modelo
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    # Entrenar el modelo
    model.fit(
        X_train,
        y_train_binary,
        epochs=10,
        batch_size=32,
        validation_split=0.2,
        class_weight=class_weight_dict,
        verbose=1
    )

    models[cls] = model

class_scores = []
for cls in unique_classes:
    print(f"Prediciendo para la clase {cls}...")
    class_scores.append(models[cls].predict(X_test, verbose=0).flatten())

class_scores = np.array(class_scores)

y_pred = np.argmax(class_scores, axis=0)

# Evaluar el modelo
print("Reporte de clasificación:")
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

etiquetas = ['NV', 'MEL', 'BKL', 'BCC', 'AKIEC', 'VASC', 'DF']

C = confusion_matrix(y_test, y_pred)
sns.heatmap(C, annot=True, fmt='d', cmap='Greens', xticklabels=np.unique(etiquetas), yticklabels=np.unique(etiquetas))
plt.title('Matriz de confusión')
plt.xlabel('Etiquetas')
plt.ylabel('Etiquetas');

"""*   Accuracy extremadamente bajo (3%): El modelo no logra distinguir adecuadamente las clases, reflejado en valores muy bajos de precisión, recall y F1-score en la mayoría de las clases.
*   El enfoque OVR mejora considerablemente con PCA, pasando de un modelo que no clasifica nada a uno funcional con un accuracy del 73%. Esto evidencia que la reducción de dimensionalidad no solo simplifica el problema, sino que permite al modelo enfocar su capacidad en los patrones más relevantes. Sin embargo, se requiere balancear las clases.

## Conclusiones

*   El dataset era muy grande y contenía muchas imágenes, por lo que se necesitaba una GPU mediante las librerías CuML y TensorFlow, además de un hardware robusto. Asimismo, se intentó usar GridSearch para encontrar los hiperparámetros óptimos, pero el hardware nuevamente fue un limitante, por lo que se decidió no utilizarlo. Esto generó una disminución en la accuracy o recall del modelo de clasificación, lo cual se reflejó aún más al trabajar con las imágenes originales.
*  El clasificador que tuvo mejor comportamiento fue el de SVM con PCA; el SVM con PCA supera a la Regresión Logística Softmax y OVR debido a su capacidad para manejar fronteras no lineales, reducir ruido con PCA y evitar el sobreajuste en escenarios complejos o con alta dimensionalidad.
"""